{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference with InterpretML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functions import df_stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "#alt.renderers.enable('notebook')\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Set is large, a smaller random sample will be sufficient for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=0 # set random state\n",
    "\n",
    "path = 'data/'\n",
    "df_raw = pd.read_csv(path+'train.csv').sample(100000, random_state=SEED)\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_stats(df):\n",
    "    \n",
    "    stats = pd.DataFrame(index=list(df))\n",
    "    stats['DataTypes'] = df.dtypes\n",
    "    stats['MissingPct'] = df.isnull().sum()/df.shape[0]*100\n",
    "    stats['NUnique'] = df.nunique().astype(float)\n",
    "    stats.reset_index().to_excel('stats.xlsx')\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stats(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features at hand:\n",
    "* `ID` is completely unique and object type, it won't be used;\n",
    "* `datetime` contains detailed time and can be preprocessed in different ways to maximize its utility;\n",
    "* `siteid`, `offerid`, `category`, `merchant` will be used as numeric columns (even though they actually refer to high cardinality categories);\n",
    "* `countrycode`, `browserid`, `devid` are categorical and therefore will be converted to numeric.\n",
    "* `click` is the target variable which will be classified. 1 refers to instances where it was clicked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID column\n",
    "df_raw.drop(\"ID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data to train (70%) and holdout (30%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_h, y_t, y_h = train_test_split(\n",
    "    df_raw.drop('click',axis=1),\n",
    "    df_raw['click'],\n",
    "    test_size=0.3,\n",
    "    random_state=SEED\n",
    ")\n",
    "X_t.shape, X_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#themes = ['dark', 'default', 'fivethirtyeight', 'ggplot2', 'latimes', 'none', 'opaque', 'quartz', 'vox']\n",
    "alt.themes.enable('vox')\n",
    "\n",
    "y = 'click'\n",
    "df_temp = X_t.copy()\n",
    "df_temp[y] = y_t\n",
    "\n",
    "for x in ['offerid', 'category', 'merchant', 'siteid']:\n",
    "\n",
    "    chart = alt.Chart(df_temp).transform_density(\n",
    "        density=x,\n",
    "        as_=[x,'density'],\n",
    "        groupby=[y],\n",
    "        counts = False\n",
    "    ).mark_area(fillOpacity=0.3).encode(\n",
    "        x=x+':Q',\n",
    "        y='density:Q',\n",
    "        color=alt.Color(y+':N', scale=alt.Scale(scheme='set1')),\n",
    "        stroke=y+':N'\n",
    "    ).properties(width=800, height=200)\n",
    "    display(chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features can be visualized with a heatmap\n",
    "* https://altair-viz.github.io/gallery/layered_heatmap_text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['countrycode', 'browserid', 'devid']\n",
    "y = 'click'\n",
    "\n",
    "for x in cols:\n",
    "    \n",
    "    to_plot = pd.crosstab(df_temp[x], df_temp[y]).reset_index(drop=False).melt(id_vars=[x])\n",
    "    size_t, size_h = tuple(df_temp[y].value_counts())\n",
    "    to_plot['rel_prop'] = round(to_plot.apply(\n",
    "        lambda col: col['value']/size_t if col['click']==0 else col['value']/size_h, axis=1),2)\n",
    "    \n",
    "    base = alt.Chart(to_plot).encode(\n",
    "    x=x, y='click:O'\n",
    "    ).properties(width=700, height=200)\n",
    "\n",
    "    heatmap = base.mark_rect().encode(\n",
    "        color=alt.Color('rel_prop:Q', scale=alt.Scale(scheme='viridis'))\n",
    "    )\n",
    "\n",
    "    text = base.mark_text().encode(\n",
    "        text='rel_prop:Q'\n",
    "    )\n",
    "    display(heatmap + text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `countrycode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "freq_table = pd.crosstab(df_raw['countrycode'], df_raw['click'], margins=True, dropna=False)\n",
    "freq_table/df_raw.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`countrycode` \"c\" and \"d\" may have some predictive power, as their overall frequency is rather low, but they are  clicked significantly more often than other codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`devid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = pd.crosstab(df_raw['devid'], df_raw['click'], margins=True, dropna=False)\n",
    "freq_table/df_raw.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It turns out that all of the dates are in year 2017, since year variable would have no variance, lets drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some categorical features remaining which require encoding, for that we can use ordinal encoder.\n",
    "As much as I like to use sklearn, its class OrdinalEncoder doesn't accept missing values or out of sample encoding (while transforming), so an alternative library called category_encoders is used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we will skip hyper parameter tuning and cross validation for computation reasons, but it can be easily included with sklearn classes `GridSearchCV` or `RandomSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import shap\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,use_dates = ['year', 'month', 'day', 'hour', 'minute', 'second']):\n",
    "        self._use_dates = use_dates\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def get_dates(self, obj):        \n",
    "        return pd.DatetimeIndex(obj)\n",
    "    \n",
    "    def transform(self, X , y = None ):\n",
    "\n",
    "        for spec in self._use_dates:\n",
    "            \n",
    "            command = f\"X['{spec}'] = self.get_dates(X['datetime']).{spec}\"\n",
    "            exec(command)\n",
    "        \n",
    "        X = X.drop(['datetime','year'], axis = 1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test `DateTransformer()` Class;\n",
    "* Save column list (pipeline returns numpy array and doesn't remember column names);\n",
    "* Test `OrdinalEncoder()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = X_t.head(500).copy()\n",
    "ct = DateTransformer()\n",
    "ct.fit(X=df_sample, y = None)\n",
    "a = ct.transform(df_sample)\n",
    "feature_names = list(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder(handle_unknown='value', handle_missing='value')\n",
    "d = a[['countrycode','devid', 'browserid']].head(10)\n",
    "enc.fit(d)\n",
    "enc.transform(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and fit pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm pipeline\n",
    "pipeline_lgb = Pipeline([\n",
    "    (\"transform_dates\", DateTransformer()),\n",
    "    (\"transform_cats\", OrdinalEncoder(handle_unknown='value', handle_missing='value')),\n",
    "    (\"impute\", SimpleImputer(fill_value=0)),\n",
    "    (\"model\", LGBMClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.Timestamp.now()\n",
    "pipeline_lgb.fit(X=X_t, y=y_t)\n",
    "print(pd.Timestamp.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.Timestamp.now()\n",
    "predictions_lgb = pipeline_lgb.predict(X=X_h)\n",
    "print(pd.Timestamp.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`siteid, offerid, category, merchant`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pipeline can't return missing values, because they are not supported by explainable ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EBM pipeline\n",
    "start = pd.Timestamp.now()\n",
    "pipeline_ebm = Pipeline([\n",
    "    (\"transform_dates\", DateTransformer()),\n",
    "    (\"transform_cats\", OrdinalEncoder(handle_unknown='value', handle_missing='value')),\n",
    "    (\"impute\", SimpleImputer(fill_value=0)),\n",
    "    (\"model\", ExplainableBoostingClassifier(feature_names=feature_names))\n",
    "    ])\n",
    "\n",
    "pipeline_ebm.fit(X_t, y_t)\n",
    "print(pd.Timestamp.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.Timestamp.now()\n",
    "predictions_ebm = pipeline_ebm.predict(X=X_h)\n",
    "print(pd.Timestamp.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_h, predictions_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_t, pipeline_ebm.predict(X=X_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add accuracy results from both models on training and test sets to a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgb_report = pd.DataFrame(classification_report(y_t, pipeline_lgb.predict(X=X_t), output_dict=True))\n",
    "test_lgb_report = pd.DataFrame(classification_report(y_h, predictions_lgb, output_dict=True))\n",
    "lgb_report = pd.concat([train_lgb_report,test_lgb_report])[[\"0\", \"1\"]]\n",
    "\n",
    "train_ebm_report = pd.DataFrame(classification_report(y_t, pipeline_ebm.predict(X=X_t), output_dict=True))\n",
    "test_ebm_report = pd.DataFrame(classification_report(y_h, predictions_ebm, output_dict=True))\n",
    "ebm_report = pd.concat([train_ebm_report,test_ebm_report])[[\"0\", \"1\"]]\n",
    "\n",
    "full_report = pd.concat([lgb_report, ebm_report], axis=1)\n",
    "\n",
    "index = list(full_report.index)\n",
    "multi_index = []\n",
    "\n",
    "for i in range(0, len(index)):\n",
    "    if i < 4:\n",
    "        multi_index.append(f'training set: {index[i]}')\n",
    "    else:\n",
    "        multi_index.append(f'holdout set: {index[i]}')\n",
    "\n",
    "full_report.index = multi_index\n",
    "full_report.columns = ['0 (lgb)', '1 (lgb)', '0 (ebm)', '1 (ebm)']\n",
    "full_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an imbalanced classification task and the majority class (0) are nearly perfectly predicted, and minority class (1) is predicted reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model interpretation\n",
    "When building an interpretable we need to think about how we will construct our pipeline as well. Model inference libraries are not suited for all inputs.\n",
    "* SHAP tree explainer supports missing values but doesn't support sklearn Pipeline or non numeric values. When building a gradient boosting model, tree explainer lets use the benefits of computing shap values fast.\n",
    "* Interpret_ML doesn't support missing values, but can be fitted on the sklearn Pipeline. Does it support categorical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline consists of three steps, but we only need the first two, which transform data: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library also provides tools like `ClassHistogram`, which can be useful to conduct EDA, but you may need to sample data. Since it's interactive and runs under `Plotly` it may quickly become something that takes long to load as data grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data sets with pipeline preprocessing\n",
    "X_t_prep = pd.DataFrame(data=pipeline_lgb[0:3].transform(X_t), columns=feature_names)\n",
    "X_h_prep = pd.DataFrame(data=pipeline_lgb[0:3].transform(X_h), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import show\n",
    "from interpret.data import ClassHistogram\n",
    "\n",
    "hist = ClassHistogram().explain_data(X_t_prep, y_t, name = 'Train Data')\n",
    "show(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP\n",
    "start = pd.Timestamp.now()\n",
    "explainer = shap.TreeExplainer(pipeline_lgb['model'])\n",
    "shap_values = explainer.shap_values(X_t_prep)\n",
    "print(pd.Timestamp.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 90\n",
    "\n",
    "ebm_global = pipeline_ebm['model'].explain_global()\n",
    "show(ebm_global)\n",
    "\n",
    "shap.summary_plot(shap_values[0], X_t_prep, plot_type=\"bar\", plot_size=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(ebm_global)\n",
    "shap.dependence_plot(ind=\"countrycode\", shap_values=shap_values[0], features=X_t_prep, interaction_index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = [69950]\n",
    "# InterpretML\n",
    "print(f\"True value: {y_t.iloc[ind]}\")\n",
    "ebm_local = pipeline_ebm['model'].explain_local(X_t_prep.iloc[ind], y_t.iloc[ind], name='Local')\n",
    "show(ebm_local)\n",
    "# SHAP\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][ind,:], X_t_prep.iloc[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(shap_values[0][ind,:], columns = list(X_t_prep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t_prep.iloc[ind]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
